---
title: "GEFCom2017 tutorial: part 1"
author: "Cameron Roach"
#date: 2017-12-16T10:23:00-00:00
date: 2018-09-28T14:35:00
bibliography: "library.bib"
categories: ["R"]
tags: ["R", "Tutorial", "Forecasting", "Electricity"]
description: Part 1 of a tutorial on implementing my GEFCom2017 model entry for probabilistic forecasts in a hierarchical setting. Here we discuss loading data and performing cross validation.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	collapse = TRUE
)

rm(list=ls())

# library(knitr)
# library(kableExtra)
```


## Introduction

This post shows how to implement the model I propose in the paper _Reconciled boosted models for GEFCom2017 hierarchical probabilistic load forecasting_ [@Roach2018-pf].

Before starting, make sure you have the following packages installed:

* `tidyverse`
* `lubridate`
* `caret`
* `gefcom2017data`.

To install the `gefcom2017` package run:

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("camroach87/gefcom2017data")
```

We will also be using the `xgboost` package, but `caret` should automatically install this when we fit models.


### Data

The `gefcom` data frame contains the data we wish to work with. Eight bottom-level zones and two aggregated zones are included.

```{r}
library(caret)
library(tidyverse)
library(lubridate)
library(gefcom2017data)

gefcom
```

A week of electricity demand data for each zone is shown below.

```{r}
gefcom %>%
  filter(ts >= dmy("1/2/2017"),
         ts < dmy("8/2/2017")) %>%
  ggplot(aes(x = ts, y = demand, colour = zone)) +
  geom_line() +
  theme(legend.position = "bottom")
```

We will nest the data frame by zone to make it easier to work with. This will make storing models and results easier.

```{r}
gefcom <- gefcom %>%
  group_by(zone) %>%
  nest()

gefcom
```


### Preprocessing

Electricity demand is typically affected by weather conditions on previous days. To obtain more realistic demand simulations we include lags for the last 72 periods. We can use the `add_lags` function to do this. You can examine the new structure of the data for one of the zones below.

```{r}
add_lags <- function(data, variables, lags = 1) {
  for (iV in variables) {
    for (iL in lags) {
      lag_name <- paste0(iV, "_lag", iL)
      data[[lag_name]] <- lag(data[[iV]], iL)
    }
  }
  
  # Remove NAs in first 72 rows due to lags
  data <- data %>% 
    slice((max(lags)+1):nrow(data))
  
  data
}

gefcom <- gefcom %>%
  mutate(data = map(data, add_lags, variables = c("drybulb", "dewpnt"), 
                    lags = 1:72))

# gefcom %>%
#   filter(zone == "SEMASS") %>%
#   unnest() %>%
#   head() %>%
#   kable() %>%
#   kable_styling() %>%
#   scroll_box(width = "100%")

gefcom %>%
  filter(zone == "SEMASS") %>%
  unnest()
```

[That's a lot of lags!](https://www.youtube.com/watch?v=R2vBZuLI3oI) Thankfully we can perform automatic feature selection and regularisation using the `caret` and `XGBoost` packages.

As a final step we will set the `holiday` variable to `FALSE` on those occasions it falls on a weekend. It's pretty clear from the boxplots below that while demand does drop a lot during weekday holidays, the same effect is not observed on weekends. Electricity demand seems to be about the same on weekends regardless of whether it is a public holiday or not.^[Note that the wider whiskers and extra outliers for non-holidays is a reflection of having much more data for those days. There are few public holidays falling on weekends relative to non-holidays.]

```{r}
gefcom %>% 
  unnest() %>% 
  ggplot(aes(x = weekend, y = demand, colour = holiday)) + 
  geom_boxplot() + 
  facet_wrap(~zone, scales = "free_y")

gefcom <- gefcom %>% 
  unnest() %>% 
  mutate(holiday = if_else(weekend==TRUE, FALSE, holiday)) %>% 
  group_by(zone) %>%
  nest()
```


## Fitting models

### Cross validation

Here we'll use `caret` to train and carry out cross-validation for our models.^[I'm only doing 5-fold cross-validation. I suspect you'd get better results carrying out time series cross-validation, but I' doubt the end results would vary by too much'd be suprised if the results were dramatically different.] You can do your own cross-validation by changing the `method` argument in `trainControl` and updating the list that defines `xgb_grid` to include extra hyperparameter values. The `cross_df` function returns a dataframe with all parameter combinations, so it is an easy way to create a grid for parameter tuning.

Note that we have set `allowParallel` to FALSE in `trainControl`. This is to avoid conflicts with XGBoost which automatically does parallel processing when the `nthread` argument is not set. I prefer using XGBoost's parallelisation rather than `caret`'s as I found myself running into RAM usage issues with `caret`.

```{r}
xgb_ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  allowParallel = FALSE,
  returnData = FALSE,
  trim = TRUE,
  returnResamp = "none",
  savePredictions = "none"
)

xgb_grid <- list(
  nrounds = 50,
  alpha = c(0, 50),
  lambda = c(0, 50),
  eta = 0.1
) %>%
  cross_df() %>% 
  as.data.frame()  # caret throws a weird error without

fit_boosted_model <- function(data, zone) {
  cat("Carrying out cross-validation for zone", zone, "...\n")
  
  data %>%
    select(demand, hour, day_of_year, day_of_week, holiday, trend,
           starts_with("drybulb"), starts_with("dewpnt")) %>%
    na.omit() %>%
    train(demand ~ .,
          data = .,
          method="xgbLinear",
          trControl = xgb_ctrl,
          tuneGrid = xgb_grid)
}
```

Before fitting our models we need to split data into training and test data sets. I'm using a month of data for our test set and all remaining historical data for training.

```{r}
# Create train and test data sets
gefcom <- gefcom %>% 
  mutate(train = map(data, function(x) filter(x, x$date < dmy("1/4/2017"))),
         test = map(data, function(x) filter(x, x$date >= dmy("1/4/2017")))) %>% 
  select(-data)

# Train models
gefcom <- gefcom %>%
  mutate(caret = map2(train, zone, fit_boosted_model))
```


### Best fit


### Vanilla benchmark model


## Next time

Next we will look at hierarchical reconciliation.

Here is all the code we have created up to this point.

```{r}

```


<!-- ## Inputs -->

<!-- We define some important dates. These will be needed when calculating trends and forecasts. -->

<!-- ```{r} -->
<!-- library(lubridate) -->

<!-- fcst_start_dt <- dmy_hms("1/6/2016 00:00:00") -->
<!-- fcst_end_dt <- dmy_hms("1/7/2016 00:00:00") -->
<!-- hist_start_dt <- dmy_hms("1/5/2016 00:00:00") -->
<!-- hist_end_dt <- dmy_hms("1/6/2016 00:00:00") -->
<!-- ``` -->

<!-- ## Data -->

<!-- If you already have your data in an appropriate format (see table below) and you do not want to use the same predictors that I have you can skip ahead to fitting the model. -->

<!-- ### Data structure -->

<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library(caret) -->
<!-- library(gefcom2017data) -->

<!-- gefcom -->
<!-- ``` -->


<!-- ### Feature engineering -->

<!-- Date time, day of week and other features are added. -->

<!-- ```{r} -->
<!-- elec_df <- elec_df %>%  -->
<!--   mutate(dt = ymd_h(paste(date, hour)), -->
<!--          day_of_week = wday(dt), -->
<!--          day_of_year = yday(dt)) -->
<!-- ``` -->

<!-- Our trend is based off the `hist_start_date` variable. While the actual start date for each zone's historical data may differ, as long as we are consistent in using this basis when predicting this will be fine. -->

<!-- ```{r} -->
<!-- elec_df <- elec_df %>%  -->
<!--   mutate(trend = as.numeric(dt - hist_start_dt)) -->
<!-- ``` -->

<!-- We now need to add lags to our data frame. The `get_lagged_vars` function allows us to pass a vector of integers with the required lags and returns a data frame with new lag columns. We group and nest the data frame by zones to prevent lags including values from other zones. -->

<!-- ```{r} -->
<!-- elec_df <- elec_df %>% -->
<!--   group_by(zone) %>% -->
<!--   nest() %>% -->
<!--   mutate(data = map(data, get_lagged_vars, -->
<!--                     variables = c("dry_bulb", "dew_pnt"), lags = 1:72)) -->
<!-- ``` -->

<!-- Here is how the data should look for one of the zones when we unnest it. -->

<!-- ```{r} -->
<!-- elec_df %>%  -->
<!--   filter(zone == "SEMASS") %>%  -->
<!--   unnest() %>%  -->
<!--   head() %>%  -->
<!--   kable() %>%  -->
<!--   kable_styling() %>% -->
<!--   scroll_box(width = "100%") -->
<!-- ``` -->


<!-- ## Fitting the model -->

<!-- I'm only fitting one model with an L1 penalty of 50 for each zone. You can do your own cross-validation by changing the `method` argument in `trainControl` and updating the list that defines `xgb_grid` to include extra hyperparameter values. The `cross_df` function returns a dataframe with all parameter combinations, so it is an easy way to create a grid for parameter tuning. -->

<!-- Note that we have set `allowParallel` to FALSE in `trainControl`. This is to avoid issues with XGBoost which automatically does parallel processing when the `nthread` argument is not set. I prefer using XGBoost's parallelisation rather than `caret`'s as I found myself running into RAM usage issues with `caret`. -->

<!-- ```{r} -->
<!-- xgb_ctrl <- trainControl( -->
<!--   method = "none", -->
<!--   allowParallel = FALSE, -->
<!--   returnData = FALSE, -->
<!--   trim = TRUE, -->
<!--   returnResamp = "none", -->
<!--   savePredictions = "none" -->
<!-- ) -->

<!-- xgb_grid <- list( -->
<!--   nrounds = 200, -->
<!--   alpha = 50, -->
<!--   lambda = 0, -->
<!--   eta = 0.1 -->
<!-- ) %>%  -->
<!--   cross_df() -->

<!-- fit_boosted_model <- function(train_df) { -->
<!--   train_df %>% -->
<!--     select(demand, hour, day_of_year, day_of_week, holiday_flag, trend, -->
<!--            starts_with("dry_bulb"), starts_with("dew_pnt")) %>% -->
<!--     na.omit() %>%  -->
<!--     train(demand ~ . , -->
<!--           data = ., -->
<!--           method="xgbLinear", -->
<!--           trControl = xgb_ctrl, -->
<!--           tuneGrid = xgb_grid) -->
<!-- } -->

<!-- elec_df <- elec_df %>%  -->
<!--   mutate(fit = map(data, fit_boosted_model)) -->
<!-- ``` -->


<!-- ## Simulating in a hierarchy -->

<!-- Now that we have fit our demand models we need to create weather and residual simulations. The weather simulations will be fed into the demand model to predict the expected demand. Our goal is to create demand simulations so we also need to add residuals back in (or we are only simulating the conditional mean). So the final step is to add residual simulations to the expected demand to create true demand simulations. -->

<!-- Unfortunately - there's a lot of code refactoring in the package that I need to do! I'll update this post asap with a simple approach to weather simulation and hierarchical reconciliation. -->

<!-- ### Weather simulations -->

<!-- ```{r} -->
<!-- # elec_df %>%  -->
<!-- #   mutate(weather_sims = map(data, shuffle_weather, fcst_start_dt, fcst_end_dt, trend_start)) -->
<!-- ``` -->

<!-- ### Residual resampling -->

<!-- ```{r} -->

<!-- ``` -->


<!-- ## Hierarchical reconciliation -->


## References {-}