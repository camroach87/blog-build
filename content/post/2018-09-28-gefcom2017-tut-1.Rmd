---
title: "GEFCom2017 tutorial: part 1"
author: "Cameron Roach"
#date: 2017-12-16T10:23:00-00:00
date: 2018-09-28T14:35:00
categories: ["R"]
tags: ["R", "Tutorial", "Forecasting", "Electricity"]
description: Part 1 of a tutorial on implementing my GEFCom2017 model entry for probabilistic forecasts in a hierarchical setting. Here we discuss loading data and performing cross validation.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	collapse = TRUE
)

rm(list=ls())

# library(knitr)
# library(kableExtra)
```


## Introduction

This post shows how to implement the model I propose in the paper "Reconciled boosted models for GEFCom2017 hierarchical probabilistic load forecasting".

Before starting, make sure you have the following packages installed:

* `tidyverse`
* `lubridate`
* `caret`
* `gefcom2017data`.

To install the `gefcom2017` package run:

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("camroach87/gefcom2017data")
```

We will also be using the `xgboost` package, but `caret` should automatically install this when we fit models.

<!-- ## Inputs -->

<!-- We define some important dates. These will be needed when calculating trends and forecasts. -->

<!-- ```{r} -->
<!-- library(lubridate) -->

<!-- fcst_start_dt <- dmy_hms("1/6/2016 00:00:00") -->
<!-- fcst_end_dt <- dmy_hms("1/7/2016 00:00:00") -->
<!-- hist_start_dt <- dmy_hms("1/5/2016 00:00:00") -->
<!-- hist_end_dt <- dmy_hms("1/6/2016 00:00:00") -->
<!-- ``` -->

<!-- ## Data -->

<!-- If you already have your data in an appropriate format (see table below) and you do not want to use the same predictors that I have you can skip ahead to fitting the model. -->

<!-- ### Data structure -->

<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library(caret) -->
<!-- library(gefcom2017data) -->

<!-- gefcom -->
<!-- ``` -->


<!-- ### Feature engineering -->

<!-- Date time, day of week and other features are added. -->

<!-- ```{r} -->
<!-- elec_df <- elec_df %>%  -->
<!--   mutate(dt = ymd_h(paste(date, hour)), -->
<!--          day_of_week = wday(dt), -->
<!--          day_of_year = yday(dt)) -->
<!-- ``` -->

<!-- Our trend is based off the `hist_start_date` variable. While the actual start date for each zone's historical data may differ, as long as we are consistent in using this basis when predicting this will be fine. -->

<!-- ```{r} -->
<!-- elec_df <- elec_df %>%  -->
<!--   mutate(trend = as.numeric(dt - hist_start_dt)) -->
<!-- ``` -->

<!-- We now need to add lags to our data frame. The `get_lagged_vars` function allows us to pass a vector of integers with the required lags and returns a data frame with new lag columns. We group and nest the data frame by zones to prevent lags including values from other zones. -->

<!-- ```{r} -->
<!-- elec_df <- elec_df %>% -->
<!--   group_by(zone) %>%  -->
<!--   nest() %>%  -->
<!--   mutate(data = map(data, get_lagged_vars, -->
<!--                     variables = c("dry_bulb", "dew_pnt"), lags = 1:72)) -->
<!-- ``` -->

<!-- Here is how the data should look for one of the zones when we unnest it. -->

<!-- ```{r} -->
<!-- elec_df %>%  -->
<!--   filter(zone == "SEMASS") %>%  -->
<!--   unnest() %>%  -->
<!--   head() %>%  -->
<!--   kable() %>%  -->
<!--   kable_styling() %>% -->
<!--   scroll_box(width = "100%") -->
<!-- ``` -->


<!-- ## Fitting the model -->

<!-- I'm only fitting one model with an L1 penalty of 50 for each zone. You can do your own cross-validation by changing the `method` argument in `trainControl` and updating the list that defines `xgb_grid` to include extra hyperparameter values. The `cross_df` function returns a dataframe with all parameter combinations, so it is an easy way to create a grid for parameter tuning. -->

<!-- Note that we have set `allowParallel` to FALSE in `trainControl`. This is to avoid issues with XGBoost which automatically does parallel processing when the `nthread` argument is not set. I prefer using XGBoost's parallelisation rather than `caret`'s as I found myself running into RAM usage issues with `caret`. -->

<!-- ```{r} -->
<!-- xgb_ctrl <- trainControl( -->
<!--   method = "none", -->
<!--   allowParallel = FALSE, -->
<!--   returnData = FALSE, -->
<!--   trim = TRUE, -->
<!--   returnResamp = "none", -->
<!--   savePredictions = "none" -->
<!-- ) -->

<!-- xgb_grid <- list( -->
<!--   nrounds = 200, -->
<!--   alpha = 50, -->
<!--   lambda = 0, -->
<!--   eta = 0.1 -->
<!-- ) %>%  -->
<!--   cross_df() -->

<!-- fit_boosted_model <- function(train_df) { -->
<!--   train_df %>% -->
<!--     select(demand, hour, day_of_year, day_of_week, holiday_flag, trend, -->
<!--            starts_with("dry_bulb"), starts_with("dew_pnt")) %>% -->
<!--     na.omit() %>%  -->
<!--     train(demand ~ . , -->
<!--           data = ., -->
<!--           method="xgbLinear", -->
<!--           trControl = xgb_ctrl, -->
<!--           tuneGrid = xgb_grid) -->
<!-- } -->

<!-- elec_df <- elec_df %>%  -->
<!--   mutate(fit = map(data, fit_boosted_model)) -->
<!-- ``` -->


<!-- ## Simulating in a hierarchy -->

<!-- Now that we have fit our demand models we need to create weather and residual simulations. The weather simulations will be fed into the demand model to predict the expected demand. Our goal is to create demand simulations so we also need to add residuals back in (or we are only simulating the conditional mean). So the final step is to add residual simulations to the expected demand to create true demand simulations. -->

<!-- Unfortunately - there's a lot of code refactoring in the package that I need to do! I'll update this post asap with a simple approach to weather simulation and hierarchical reconciliation. -->

<!-- ### Weather simulations -->

<!-- ```{r} -->
<!-- # elec_df %>%  -->
<!-- #   mutate(weather_sims = map(data, shuffle_weather, fcst_start_dt, fcst_end_dt, trend_start)) -->
<!-- ``` -->

<!-- ### Residual resampling -->

<!-- ```{r} -->

<!-- ``` -->


<!-- ## Hierarchical reconciliation -->

