---
title: "GEFCom2017 tutorial: part 1"
author: "Cameron Roach"
#date: 2017-12-16T10:23:00-00:00
date: 2018-09-28T14:35:00
bibliography: "library.bib"
categories: ["R"]
tags: ["R", "Tutorial", "Forecasting", "Electricity"]
description: Part 1 of a tutorial on implementing my GEFCom2017 model entry for probabilistic forecasts in a hierarchical setting. Here we discuss loading data and performing cross validation.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	collapse = TRUE
)

rm(list=ls())

# library(knitr)
# library(kableExtra)
```


## Introduction

This post shows how to implement the model I propose in the paper _Reconciled boosted models for GEFCom2017 hierarchical probabilistic load forecasting_ [@Roach2018-pf].

Before starting, make sure you have the following packages installed:

* `tidyverse`
* `lubridate`
* `caret`
* `gefcom2017data`.

To install the `gefcom2017` package run:

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("camroach87/gefcom2017data")
```

We will also be using the `xgboost` package, but `caret` should automatically install this when we fit models.


### Data

The `gefcom` data frame contains the data we wish to work with. Eight bottom-level zones and two aggregated zones are included.

```{r}
library(caret)
library(tidyverse)
library(lubridate)
library(gefcom2017data)

gefcom
```

A week of electricity demand data for each zone is shown below.

```{r}
gefcom %>%
  filter(ts >= dmy("1/2/2017"),
         ts < dmy("8/2/2017")) %>%
  ggplot(aes(x = ts, y = demand, colour = zone)) +
  geom_line() +
  theme(legend.position = "bottom")
```

We will nest the data frame by zone to make it easier to work with. This will make storing models and results easier.

```{r}
gefcom <- gefcom %>%
  group_by(zone) %>%
  nest()

gefcom
```


### Preprocessing

Electricity demand is typically affected by weather conditions on previous days. To obtain better fitting models we include lags for the last 72 periods. We can use the `add_lags` function to do this. You can examine the new structure of the data for one of the zones below.

```{r}
add_lags <- function(data, variables, lags = 1) {
  for (iV in variables) {
    for (iL in lags) {
      lag_name <- paste0(iV, "_lag", iL)
      data[[lag_name]] <- lag(data[[iV]], iL)
    }
  }
  
  # Remove NAs in early rows due to lags
  data <- data %>% 
    slice((max(lags)+1):nrow(data))
  
  data
}

gefcom <- gefcom %>%
  mutate(data = map(data, add_lags, variables = c("drybulb", "dewpnt"), 
                    lags = 1:72))

gefcom %>%
  filter(zone == "SEMASS") %>%
  unnest()
```

[That's a lot of lags!](https://www.youtube.com/watch?v=R2vBZuLI3oI) Thankfully we can perform automatic feature selection and regularisation using the `caret` and `XGBoost` packages.

As a final step we will set the `holiday` variable to `FALSE` on those occasions a holiday falls on a weekend. It's pretty clear from the boxplots below that while demand does drop a lot during weekday holidays, the same effect is not observed on weekends. Electricity demand seems to be about the same on weekends regardless of whether it is a public holiday or not.^[Note that the wider whiskers and extra outliers for non-holidays is a reflection of having much more data for those days. There are few public holidays falling on weekends relative to non-holidays.]

```{r}
gefcom %>% 
  unnest() %>% 
  ggplot(aes(x = weekend, y = demand, fill = holiday)) + 
  geom_boxplot(outlier.shape = "o", outlier.alpha = 0.5) +
  facet_wrap(~zone, scales = "free_y") +
  theme(legend.position = "bottom")

gefcom <- gefcom %>% 
  unnest() %>% 
  mutate(holiday = if_else(weekend==TRUE, FALSE, holiday)) %>% 
  group_by(zone) %>%
  nest()
```


## Fitting models

Here we'll use `caret` to train and carry out cross-validation for our models.^[I'm only doing 5-fold cross-validation. I suspect you'd get slightly better results carrying out time series cross-validation.] You can do your own cross-validation by changing the `method` argument in `trainControl` and updating the list that defines `xgb_grid` to include extra hyperparameter values. The `cross_df` function returns a data frame with all parameter combinations, so it is an easy way to create a grid for parameter tuning.

Note that we have set `allowParallel` to FALSE in `trainControl`. This is to avoid conflicts with XGBoost which automatically does parallel processing when the `nthread` argument is not set. I prefer using XGBoost's parallelisation rather than `caret`'s as I found myself running into RAM usage issues with `caret`.

I've also created a function to fit our benchmark model, Tao's Vanilla energy model [@Hong2010-cy]. It has been used in several GEFComs (including GEFCom2017) and is the go-to energy forecasting benchmark model due to its ubiquity and ease of use. Note that we do not do any cross-validation or feature selection for this model - we use it as is.

```{r}
xgb_ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  allowParallel = FALSE,
  returnData = FALSE,
  trim = TRUE,
  returnResamp = "none",
  savePredictions = "none"
)

xgb_grid <- list(
  nrounds = 50,
  alpha = c(0, 50),
  lambda = c(0, 50),
  eta = 0.1
) %>%
  cross_df() %>% 
  as.data.frame()  # caret throws a weird error without

fit_boosted_model <- function(data, zone, ...) {
  cat("Carrying out cross-validation for zone", zone, "...\n")
  
  data %>%
    select(demand, hour, day_of_year, day_of_week, holiday, trend,
           starts_with("drybulb"), starts_with("dewpnt")) %>%
    na.omit() %>%
    sample_frac(...) %>% 
    train(demand ~ .,
          data = .,
          method="xgbLinear",
          trControl = xgb_ctrl,
          tuneGrid = xgb_grid)
}

fit_vanilla_model <- function(data, zone, ...) {
  cat("Fitting vanilla model for zone", zone, "...\n")
  
  data %>%
    select(demand, hour, month, day_of_week, trend, drybulb) %>%
    na.omit() %>%
    sample_frac(...) %>% 
    train(demand ~ day_of_week*hour + poly(drybulb, 3, raw = TRUE)*month +
            poly(drybulb, 3, raw = TRUE)*hour + trend,
          data = .,
          method = "lm",
          trControl = xgb_ctrl)
}
```

Before fitting our models we need to split data into training and test data sets. I'm using a month of data for our test set and all remaining historical data for training. Note that I've added a `frac` parameter to subsample the training data.^[You will also notice that I'm creating different training sets for our boosted and vanilla models. It's pretty trivial to make these the same but I don't want to bog this tutorial down in minor details so I'll leave this as an exercise! In practice you'll want to use all the data so it's a moot point.] This is purely to reduce the computation time when I compile this blog. You can remove this or set it to another value depending on how you want to compromise between speed and performance.

```{r}
# Create train and test data sets
gefcom <- gefcom %>% 
  mutate(train = map(data, function(x) filter(x, x$date < dmy("1/4/2017"))),
         test = map(data, function(x) filter(x, x$date >= dmy("1/4/2017")))) %>% 
  select(-data)

# Train models
gefcom <- gefcom %>%
  mutate(caret_xgb = map2(train, zone, fit_boosted_model, size = 0.01),
         caret_vanilla = map2(train, zone, fit_vanilla_model, size = 0.01))
```


### Best fit

Ok great, we've done our cross-validation (using 1% of the available data :-O) and now it's time to pick the best model. Once again, `purrr`'s `map` function comes to the fore.

```{r}
gefcom <- gefcom %>% 
  mutate(final_xgb = map(caret_xgb, "finalModel"),
         final_vanilla = map(caret_vanilla, "finalModel"))
```

Our data frame now contains data, cross-validation results and the final models for each zone.

```{r}
gefcom
```

## Next time

In the next post we will perform weather and residual simulations before using hierarchical reconciliation to create cohesive probabilistic forecasts.

## References {-}