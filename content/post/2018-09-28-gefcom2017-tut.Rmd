---
title: "My GEFCom2017 paper"
author: "Cameron Roach"
#date: 2017-12-16T10:23:00-00:00
date: "`r format(Sys.time(), '%Y-%m-%dT%H:%M:00')`"
categories: ["R"]
tags: ["R", "Tutorial", "Forecasting", "Electricity"]
description: Tutorial on implementing my GEFCom2017 model entry.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	collapse = TRUE
)

rm(list=ls())
```


__Caveat Emptor!:__ This is a _work in progress_! I'm refactoring the original code I wrote. Trying to find a nice balance between wrapping functions up in the `gefcom2017` package and using existing R packages such as `purrr`. This post has been uploaded early so I can include a link in the paper.

## Introduction

This post shows how to implement the model I propose in the paper "Reconciled boosted models for GEFCom2017 hierarchical probabilistic load forecasting".

Before starting, make sure you have the following packages installed:

* `tidyverse`
* `lubridate`
* `caret`
* `gefcom2017`.

To install the `gefcom2017` package run:

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("camroach87/gefcom2017")
```


We will also be using the `xgboost` package, but `caret` should automatically install this when we fit models.

## Inputs

We define some important dates. These will be needed when calculating trends and forecasts.

```{r}
library(lubridate)

fcst_start_dt <- dmy_hms("1/6/2016 00:00:00")
fcst_end_dt <- dmy_hms("1/7/2016 00:00:00")
hist_start_dt <- dmy_hms("1/5/2016 00:00:00")
hist_end_dt <- dmy_hms("1/6/2016 00:00:00")
```


## Data

If you already have your data in an appropriate format (see table below) and you do not want to use the same predictors that I have you can skip ahead to fitting the model.

### Initial structure

Data can be downloaded from __TODO__. This is a cleaned version of the data used in the competition. Ensure you have your data in a tidy format as shown below. If you only have one temperature variable that is fine.

```{r}
library(tidyverse)
library(lubridate)
library(knitr)
library(kableExtra)
library(caret)
library(gefcom2017)

elec_df <- read_csv("data/2018-09-28-gefcom2017-tut.csv")

elec_df %>% 
  head() %>% 
  kable() %>% 
  kable_styling()
```


### Feature engineering

Date time, day of week and other features are added.

```{r}
elec_df <- elec_df %>% 
  mutate(dt = ymd_h(paste(date, hour)),
         day_of_week = wday(dt),
         day_of_year = yday(dt))
```

Our trend is based off the `hist_start_date` variable. While the actual start date for each zone's historical data may differ, as long as we are consistent in using this basis when predicting this will be fine.

```{r}
elec_df <- elec_df %>% 
  mutate(trend = as.numeric(dt - hist_start_dt))
```

We now need to add lags to our data frame. The `get_lagged_vars` function allows us to pass a vector of integers with the required lags and returns a data frame with new lag columns. We group and nest the data frame by zones to prevent lags including values from other zones.

```{r}
elec_df <- elec_df %>%
  group_by(zone) %>% 
  nest() %>% 
  mutate(data = map(data, get_lagged_vars,
                    variables = c("dry_bulb", "dew_pnt"), lags = 1:72))
```

Here is how the data should look for one of the zones when we unnest it.

```{r}
elec_df %>% 
  filter(zone == "SEMASS") %>% 
  unnest() %>% 
  head() %>% 
  kable() %>% 
  kable_styling() %>%
  scroll_box(width = "100%")
```


## Fitting the model

I'm only fitting one model with an L1 penalty of 50 for each zone. You can do your own cross-validation by changing the `method` argument in `trainControl` and updating the list that defines `xgb_grid` to include extra hyperparameter values. The `cross_df` function returns a dataframe with all parameter combinations, so it is an easy way to create a grid for parameter tuning.

Note that `caret`'s `train` function has had the number of threads set to 1 by the `nthread` argument. This is because XGBoost automatically does parallel processing. Allowing caret and XGBoost to do parallel processing at the same time causes issues.

```{r}
xgb_ctrl <- trainControl(
  method = "none",
  allowParallel = FALSE,
  returnData = FALSE,
  trim = TRUE,
  returnResamp = "none",
  savePredictions = "none"
)

xgb_grid <- list(
  nrounds = 200,
  alpha = 50,
  lambda = 0,
  eta = 0.1
) %>% 
  cross_df()

fit_boosted_model <- function(train_df) {
  train_df %>%
    select(demand, hour, day_of_year, day_of_week, holiday_flag, trend,
           starts_with("dry_bulb"), starts_with("dew_pnt")) %>%
    na.omit() %>% 
    train(demand ~ . ,
          data = .,
          method="xgbLinear",
          trControl = xgb_ctrl,
          tuneGrid = xgb_grid,
          nthread = 1) # XGBoost automatically does parallel processing
}

elec_df <- elec_df %>% 
  mutate(fit = map(data, fit_boosted_model))
```


## Simulating in a hierarchy

Now that we have fit our demand models we need to create weather and residual simulations. The weather simulations will be fed into the demand model to predict the expected demand. Our goal is to create demand simulations so we also need to add residuals back in (or we are only simulating the conditional mean). So the final step is to add residual simulations to the expected demand to create true demand simulations.

### Weather simulations

```{r}
# elec_df %>% 
#   mutate(weather_sims = map(data, shuffle_weather, fcst_start_dt, fcst_end_dt, trend_start))
```

### Residual resampling

```{r}

```


## Hierarchical reconciliation

