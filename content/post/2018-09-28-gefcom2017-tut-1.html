---
title: "GEFCom2017 tutorial: part 1"
author: "Cameron Roach"
#date: 2017-12-16T10:23:00-00:00
date: 2018-09-28T14:35:00
bibliography: "library.bib"
categories: ["R"]
tags: ["R", "Tutorial", "Forecasting", "Electricity"]
description: Part 1 of a tutorial on implementing my GEFCom2017 model entry for probabilistic forecasts in a hierarchical setting. Here we discuss loading data and performing cross validation.
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This post shows how to implement the model I propose in the paper <em>Reconciled boosted models for GEFCom2017 hierarchical probabilistic load forecasting</em> <span class="citation">(Roach 2018)</span>.</p>
<p>Before starting, make sure you have the following packages installed:</p>
<ul>
<li><code>tidyverse</code></li>
<li><code>lubridate</code></li>
<li><code>caret</code></li>
<li><code>gefcom2017data</code>.</li>
</ul>
<p>To install the <code>gefcom2017</code> package run:</p>
<pre class="r"><code>install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;camroach87/gefcom2017data&quot;)</code></pre>
<p>We will also be using the <code>xgboost</code> package, but <code>caret</code> should automatically install this when we fit models.</p>
<div id="data" class="section level3">
<h3>Data</h3>
<p>The <code>gefcom</code> data frame contains the data we wish to work with. Eight bottom-level zones and two aggregated zones are included.</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(gefcom2017data)

gefcom
## # A tibble: 1,241,710 x 15
## # Groups:   zone [10]
##    ts                  zone  demand drybulb dewpnt date        year month
##    &lt;dttm&gt;              &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt;
##  1 2017-01-01 00:00:00 NEMA…  2421.      40     37 2017-01-01  2017 Jan  
##  2 2017-01-01 01:00:00 NEMA…  2335.      39     36 2017-01-01  2017 Jan  
##  3 2017-01-01 02:00:00 NEMA…  2263.      38     35 2017-01-01  2017 Jan  
##  4 2017-01-01 03:00:00 NEMA…  2217.      37     35 2017-01-01  2017 Jan  
##  5 2017-01-01 04:00:00 NEMA…  2206.      37     35 2017-01-01  2017 Jan  
##  6 2017-01-01 05:00:00 NEMA…  2231.      37     35 2017-01-01  2017 Jan  
##  7 2017-01-01 06:00:00 NEMA…  2291.      37     34 2017-01-01  2017 Jan  
##  8 2017-01-01 07:00:00 NEMA…  2331.      36     33 2017-01-01  2017 Jan  
##  9 2017-01-01 08:00:00 NEMA…  2379.      38     32 2017-01-01  2017 Jan  
## 10 2017-01-01 09:00:00 NEMA…  2429.      40     31 2017-01-01  2017 Jan  
## # ... with 1,241,700 more rows, and 7 more variables: hour &lt;dbl&gt;,
## #   day_of_week &lt;fct&gt;, day_of_year &lt;dbl&gt;, weekend &lt;lgl&gt;,
## #   holiday_name &lt;chr&gt;, holiday &lt;lgl&gt;, trend &lt;dbl&gt;</code></pre>
<p>A week of electricity demand data for each zone is shown below.</p>
<pre class="r"><code>gefcom %&gt;%
  filter(ts &gt;= dmy(&quot;1/2/2017&quot;),
         ts &lt; dmy(&quot;8/2/2017&quot;)) %&gt;%
  ggplot(aes(x = ts, y = demand, colour = zone)) +
  geom_line() +
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2018-09-28-gefcom2017-tut-1_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We will nest the data frame by zone to make it easier to work with. This will make storing models and results easier.</p>
<pre class="r"><code>gefcom &lt;- gefcom %&gt;%
  group_by(zone) %&gt;%
  nest()

gefcom
## # A tibble: 10 x 2
##    zone       data                   
##    &lt;chr&gt;      &lt;list&gt;                 
##  1 NEMASSBOST &lt;tibble [124,171 × 14]&gt;
##  2 WCMASS     &lt;tibble [124,171 × 14]&gt;
##  3 SEMASS     &lt;tibble [124,171 × 14]&gt;
##  4 RI         &lt;tibble [124,171 × 14]&gt;
##  5 CT         &lt;tibble [124,171 × 14]&gt;
##  6 VT         &lt;tibble [124,171 × 14]&gt;
##  7 NH         &lt;tibble [124,171 × 14]&gt;
##  8 ME         &lt;tibble [124,171 × 14]&gt;
##  9 MASS       &lt;tibble [124,171 × 14]&gt;
## 10 TOTAL      &lt;tibble [124,171 × 14]&gt;</code></pre>
</div>
<div id="preprocessing" class="section level3">
<h3>Preprocessing</h3>
<p>Electricity demand is typically affected by weather conditions on previous days. To obtain more realistic demand simulations we include lags for the last 72 periods. We can use the <code>add_lags</code> function to do this. You can examine the new structure of the data for one of the zones below.</p>
<pre class="r"><code>add_lags &lt;- function(data, variables, lags = 1) {
  for (iV in variables) {
    for (iL in lags) {
      lag_name &lt;- paste0(iV, &quot;_lag&quot;, iL)
      data[[lag_name]] &lt;- lag(data[[iV]], iL)
    }
  }
  
  # Remove NAs in first 72 rows due to lags
  data &lt;- data %&gt;% 
    slice((max(lags)+1):nrow(data))
  
  data
}

gefcom &lt;- gefcom %&gt;%
  mutate(data = map(data, add_lags, variables = c(&quot;drybulb&quot;, &quot;dewpnt&quot;), 
                    lags = 1:72))

# gefcom %&gt;%
#   filter(zone == &quot;SEMASS&quot;) %&gt;%
#   unnest() %&gt;%
#   head() %&gt;%
#   kable() %&gt;%
#   kable_styling() %&gt;%
#   scroll_box(width = &quot;100%&quot;)

gefcom %&gt;%
  filter(zone == &quot;SEMASS&quot;) %&gt;%
  unnest()
## # A tibble: 124,099 x 159
##    zone  ts                  demand drybulb dewpnt date        year month
##    &lt;chr&gt; &lt;dttm&gt;               &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt;
##  1 SEMA… 2017-01-04 00:00:00  1279.      45     43 2017-01-04  2017 Jan  
##  2 SEMA… 2017-01-04 01:00:00  1223.      44     43 2017-01-04  2017 Jan  
##  3 SEMA… 2017-01-04 02:00:00  1190.      44     42 2017-01-04  2017 Jan  
##  4 SEMA… 2017-01-04 03:00:00  1192.      43     43 2017-01-04  2017 Jan  
##  5 SEMA… 2017-01-04 04:00:00  1236.      43     40 2017-01-04  2017 Jan  
##  6 SEMA… 2017-01-04 05:00:00  1362.      43     39 2017-01-04  2017 Jan  
##  7 SEMA… 2017-01-04 06:00:00  1573.      41     39 2017-01-04  2017 Jan  
##  8 SEMA… 2017-01-04 07:00:00  1692.      41     39 2017-01-04  2017 Jan  
##  9 SEMA… 2017-01-04 08:00:00  1709.      41     38 2017-01-04  2017 Jan  
## 10 SEMA… 2017-01-04 09:00:00  1711.      41     38 2017-01-04  2017 Jan  
## # ... with 124,089 more rows, and 151 more variables: hour &lt;dbl&gt;,
## #   day_of_week &lt;fct&gt;, day_of_year &lt;dbl&gt;, weekend &lt;lgl&gt;,
## #   holiday_name &lt;chr&gt;, holiday &lt;lgl&gt;, trend &lt;dbl&gt;, drybulb_lag1 &lt;dbl&gt;,
## #   drybulb_lag2 &lt;dbl&gt;, drybulb_lag3 &lt;dbl&gt;, drybulb_lag4 &lt;dbl&gt;,
## #   drybulb_lag5 &lt;dbl&gt;, drybulb_lag6 &lt;dbl&gt;, drybulb_lag7 &lt;dbl&gt;,
## #   drybulb_lag8 &lt;dbl&gt;, drybulb_lag9 &lt;dbl&gt;, drybulb_lag10 &lt;dbl&gt;,
## #   drybulb_lag11 &lt;dbl&gt;, drybulb_lag12 &lt;dbl&gt;, drybulb_lag13 &lt;dbl&gt;,
## #   drybulb_lag14 &lt;dbl&gt;, drybulb_lag15 &lt;dbl&gt;, drybulb_lag16 &lt;dbl&gt;,
## #   drybulb_lag17 &lt;dbl&gt;, drybulb_lag18 &lt;dbl&gt;, drybulb_lag19 &lt;dbl&gt;,
## #   drybulb_lag20 &lt;dbl&gt;, drybulb_lag21 &lt;dbl&gt;, drybulb_lag22 &lt;dbl&gt;,
## #   drybulb_lag23 &lt;dbl&gt;, drybulb_lag24 &lt;dbl&gt;, drybulb_lag25 &lt;dbl&gt;,
## #   drybulb_lag26 &lt;dbl&gt;, drybulb_lag27 &lt;dbl&gt;, drybulb_lag28 &lt;dbl&gt;,
## #   drybulb_lag29 &lt;dbl&gt;, drybulb_lag30 &lt;dbl&gt;, drybulb_lag31 &lt;dbl&gt;,
## #   drybulb_lag32 &lt;dbl&gt;, drybulb_lag33 &lt;dbl&gt;, drybulb_lag34 &lt;dbl&gt;,
## #   drybulb_lag35 &lt;dbl&gt;, drybulb_lag36 &lt;dbl&gt;, drybulb_lag37 &lt;dbl&gt;,
## #   drybulb_lag38 &lt;dbl&gt;, drybulb_lag39 &lt;dbl&gt;, drybulb_lag40 &lt;dbl&gt;,
## #   drybulb_lag41 &lt;dbl&gt;, drybulb_lag42 &lt;dbl&gt;, drybulb_lag43 &lt;dbl&gt;,
## #   drybulb_lag44 &lt;dbl&gt;, drybulb_lag45 &lt;dbl&gt;, drybulb_lag46 &lt;dbl&gt;,
## #   drybulb_lag47 &lt;dbl&gt;, drybulb_lag48 &lt;dbl&gt;, drybulb_lag49 &lt;dbl&gt;,
## #   drybulb_lag50 &lt;dbl&gt;, drybulb_lag51 &lt;dbl&gt;, drybulb_lag52 &lt;dbl&gt;,
## #   drybulb_lag53 &lt;dbl&gt;, drybulb_lag54 &lt;dbl&gt;, drybulb_lag55 &lt;dbl&gt;,
## #   drybulb_lag56 &lt;dbl&gt;, drybulb_lag57 &lt;dbl&gt;, drybulb_lag58 &lt;dbl&gt;,
## #   drybulb_lag59 &lt;dbl&gt;, drybulb_lag60 &lt;dbl&gt;, drybulb_lag61 &lt;dbl&gt;,
## #   drybulb_lag62 &lt;dbl&gt;, drybulb_lag63 &lt;dbl&gt;, drybulb_lag64 &lt;dbl&gt;,
## #   drybulb_lag65 &lt;dbl&gt;, drybulb_lag66 &lt;dbl&gt;, drybulb_lag67 &lt;dbl&gt;,
## #   drybulb_lag68 &lt;dbl&gt;, drybulb_lag69 &lt;dbl&gt;, drybulb_lag70 &lt;dbl&gt;,
## #   drybulb_lag71 &lt;dbl&gt;, drybulb_lag72 &lt;dbl&gt;, dewpnt_lag1 &lt;dbl&gt;,
## #   dewpnt_lag2 &lt;dbl&gt;, dewpnt_lag3 &lt;dbl&gt;, dewpnt_lag4 &lt;dbl&gt;,
## #   dewpnt_lag5 &lt;dbl&gt;, dewpnt_lag6 &lt;dbl&gt;, dewpnt_lag7 &lt;dbl&gt;,
## #   dewpnt_lag8 &lt;dbl&gt;, dewpnt_lag9 &lt;dbl&gt;, dewpnt_lag10 &lt;dbl&gt;,
## #   dewpnt_lag11 &lt;dbl&gt;, dewpnt_lag12 &lt;dbl&gt;, dewpnt_lag13 &lt;dbl&gt;,
## #   dewpnt_lag14 &lt;dbl&gt;, dewpnt_lag15 &lt;dbl&gt;, dewpnt_lag16 &lt;dbl&gt;,
## #   dewpnt_lag17 &lt;dbl&gt;, dewpnt_lag18 &lt;dbl&gt;, dewpnt_lag19 &lt;dbl&gt;,
## #   dewpnt_lag20 &lt;dbl&gt;, dewpnt_lag21 &lt;dbl&gt;, …</code></pre>
<p><a href="https://www.youtube.com/watch?v=R2vBZuLI3oI">That’s a lot of lags!</a> Thankfully we can perform automatic feature selection and regularisation using the <code>caret</code> and <code>XGBoost</code> packages.</p>
<p>As a final step we will set the <code>holiday</code> variable to <code>FALSE</code> on those occasions it falls on a weekend. It’s pretty clear from the boxplots below that while demand does drop a lot during weekday holidays, the same effect is not observed on weekends. Electricity demand seems to be about the same on weekends regardless of whether it is a public holiday or not.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<pre class="r"><code>gefcom %&gt;% 
  unnest() %&gt;% 
  ggplot(aes(x = weekend, y = demand, colour = holiday)) + 
  geom_boxplot() + 
  facet_wrap(~zone, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/post/2018-09-28-gefcom2017-tut-1_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>
gefcom &lt;- gefcom %&gt;% 
  unnest() %&gt;% 
  mutate(holiday = if_else(weekend==TRUE, FALSE, holiday)) %&gt;% 
  group_by(zone) %&gt;%
  nest()</code></pre>
</div>
</div>
<div id="fitting-models" class="section level2">
<h2>Fitting models</h2>
<div id="feature-selection" class="section level3">
<h3>Feature selection</h3>
</div>
<div id="cross-validation" class="section level3">
<h3>Cross validation</h3>
</div>
<div id="best-fit" class="section level3">
<h3>Best fit</h3>
</div>
</div>
<div id="next-time" class="section level2">
<h2>Next time</h2>
<p>Next we will look at hierarchical reconciliation.</p>
<!-- ## Inputs -->
<!-- We define some important dates. These will be needed when calculating trends and forecasts. -->
<!-- ```{r} -->
<!-- library(lubridate) -->
<!-- fcst_start_dt <- dmy_hms("1/6/2016 00:00:00") -->
<!-- fcst_end_dt <- dmy_hms("1/7/2016 00:00:00") -->
<!-- hist_start_dt <- dmy_hms("1/5/2016 00:00:00") -->
<!-- hist_end_dt <- dmy_hms("1/6/2016 00:00:00") -->
<!-- ``` -->
<!-- ## Data -->
<!-- If you already have your data in an appropriate format (see table below) and you do not want to use the same predictors that I have you can skip ahead to fitting the model. -->
<!-- ### Data structure -->
<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library(caret) -->
<!-- library(gefcom2017data) -->
<!-- gefcom -->
<!-- ``` -->
<!-- ### Feature engineering -->
<!-- Date time, day of week and other features are added. -->
<!-- ```{r} -->
<!-- elec_df <- elec_df %>%  -->
<!--   mutate(dt = ymd_h(paste(date, hour)), -->
<!--          day_of_week = wday(dt), -->
<!--          day_of_year = yday(dt)) -->
<!-- ``` -->
<!-- Our trend is based off the `hist_start_date` variable. While the actual start date for each zone's historical data may differ, as long as we are consistent in using this basis when predicting this will be fine. -->
<!-- ```{r} -->
<!-- elec_df <- elec_df %>%  -->
<!--   mutate(trend = as.numeric(dt - hist_start_dt)) -->
<!-- ``` -->
<!-- We now need to add lags to our data frame. The `get_lagged_vars` function allows us to pass a vector of integers with the required lags and returns a data frame with new lag columns. We group and nest the data frame by zones to prevent lags including values from other zones. -->
<!-- ```{r} -->
<!-- elec_df <- elec_df %>% -->
<!--   group_by(zone) %>% -->
<!--   nest() %>% -->
<!--   mutate(data = map(data, get_lagged_vars, -->
<!--                     variables = c("dry_bulb", "dew_pnt"), lags = 1:72)) -->
<!-- ``` -->
<!-- Here is how the data should look for one of the zones when we unnest it. -->
<!-- ```{r} -->
<!-- elec_df %>%  -->
<!--   filter(zone == "SEMASS") %>%  -->
<!--   unnest() %>%  -->
<!--   head() %>%  -->
<!--   kable() %>%  -->
<!--   kable_styling() %>% -->
<!--   scroll_box(width = "100%") -->
<!-- ``` -->
<!-- ## Fitting the model -->
<!-- I'm only fitting one model with an L1 penalty of 50 for each zone. You can do your own cross-validation by changing the `method` argument in `trainControl` and updating the list that defines `xgb_grid` to include extra hyperparameter values. The `cross_df` function returns a dataframe with all parameter combinations, so it is an easy way to create a grid for parameter tuning. -->
<!-- Note that we have set `allowParallel` to FALSE in `trainControl`. This is to avoid issues with XGBoost which automatically does parallel processing when the `nthread` argument is not set. I prefer using XGBoost's parallelisation rather than `caret`'s as I found myself running into RAM usage issues with `caret`. -->
<!-- ```{r} -->
<!-- xgb_ctrl <- trainControl( -->
<!--   method = "none", -->
<!--   allowParallel = FALSE, -->
<!--   returnData = FALSE, -->
<!--   trim = TRUE, -->
<!--   returnResamp = "none", -->
<!--   savePredictions = "none" -->
<!-- ) -->
<!-- xgb_grid <- list( -->
<!--   nrounds = 200, -->
<!--   alpha = 50, -->
<!--   lambda = 0, -->
<!--   eta = 0.1 -->
<!-- ) %>%  -->
<!--   cross_df() -->
<!-- fit_boosted_model <- function(train_df) { -->
<!--   train_df %>% -->
<!--     select(demand, hour, day_of_year, day_of_week, holiday_flag, trend, -->
<!--            starts_with("dry_bulb"), starts_with("dew_pnt")) %>% -->
<!--     na.omit() %>%  -->
<!--     train(demand ~ . , -->
<!--           data = ., -->
<!--           method="xgbLinear", -->
<!--           trControl = xgb_ctrl, -->
<!--           tuneGrid = xgb_grid) -->
<!-- } -->
<!-- elec_df <- elec_df %>%  -->
<!--   mutate(fit = map(data, fit_boosted_model)) -->
<!-- ``` -->
<!-- ## Simulating in a hierarchy -->
<!-- Now that we have fit our demand models we need to create weather and residual simulations. The weather simulations will be fed into the demand model to predict the expected demand. Our goal is to create demand simulations so we also need to add residuals back in (or we are only simulating the conditional mean). So the final step is to add residual simulations to the expected demand to create true demand simulations. -->
<!-- Unfortunately - there's a lot of code refactoring in the package that I need to do! I'll update this post asap with a simple approach to weather simulation and hierarchical reconciliation. -->
<!-- ### Weather simulations -->
<!-- ```{r} -->
<!-- # elec_df %>%  -->
<!-- #   mutate(weather_sims = map(data, shuffle_weather, fcst_start_dt, fcst_end_dt, trend_start)) -->
<!-- ``` -->
<!-- ### Residual resampling -->
<!-- ```{r} -->
<!-- ``` -->
<!-- ## Hierarchical reconciliation -->
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-Roach2018-pf">
<p>Roach, Cameron. 2018. “Reconciled Boosted Models for GEFCom2017 Hierarchical Probabilistic Load Forecasting.” <em>International Journal of Forecasting</em>. <a href="http://dx.doi.org/10.1016/j.ijforecast.2018.09.009" class="uri">http://dx.doi.org/10.1016/j.ijforecast.2018.09.009</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the wider whiskers and extra outliers for non-holidays is a reflection of having much more data for those days. There are few public holidays falling on weekends relative to non-holidays.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
